{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK convolution network - CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create a convolution neural network using CNTK. We then train it using 50000 examples from the CIFAR10 dataset and test it on the remaining 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "from itertools import repeat, chain\n",
    "\n",
    "from cntk.layers import Convolution, MaxPooling, AveragePooling, Dense\n",
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDef, StreamDefs\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk import Trainer\n",
    "from cntk.learner import adam_sgd, learning_rate_schedule, UnitType, momentum_schedule\n",
    "from cntk.ops import cross_entropy_with_softmax, classification_error, relu, input_variable, softmax, element_times\n",
    "from cntk.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of the CIFAR10 images are 32 by 32 with three colour channels. The dataset is also made up of 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model dimensions\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition\n",
    "We define a convolution network with 3 convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input, out_dims):\n",
    "    '''Creates convolution network\n",
    "    '''\n",
    "    net = Convolution((5,5), 32, init=glorot_uniform(), activation=relu, pad=True)(input)\n",
    "    net = MaxPooling((3,3), strides=(2,2))(net)\n",
    "\n",
    "    net = Convolution((5,5), 32, init=glorot_uniform(), activation=relu, pad=True)(net)\n",
    "    net = AveragePooling((3,3), strides=(2,2), pad=True)(net)\n",
    "\n",
    "    net = Convolution((5,5), 64, init=glorot_uniform(), activation=relu, pad=True)(net)\n",
    "    net = AveragePooling((3,3), strides=(2,2), pad=True)(net)\n",
    "    \n",
    "    net = Dense(64, init=glorot_uniform())(net)\n",
    "    net = Dense(out_dims, init=glorot_uniform(), activation=None)(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image reader\n",
    "In CNTK we need to define a reader that will read in the images and compose the data into the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(map_file, mean_file, train):\n",
    "    '''Define the reader for both training and evaluation action.\n",
    "    '''\n",
    "    if not os.path.exists(map_file) or not os.path.exists(mean_file):\n",
    "        raise RuntimeError(\"Please make sure you run the process_cifar_data notebook to prepare the data\")\n",
    "\n",
    "    # transformation pipeline for the features\n",
    "    transforms = [\n",
    "        ImageDeserializer.scale(width=IMAGE_WIDTH,\n",
    "                                height=IMAGE_HEIGHT,\n",
    "                                channels=NUM_CHANNELS,\n",
    "                                interpolations='linear'),\n",
    "        ImageDeserializer.mean(mean_file)\n",
    "    ]\n",
    "    # deserializer\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "        features=StreamDef(field='image', transforms=transforms),  # first column in map file is referred\n",
    "                                                                   # to as 'image'\n",
    "        labels=StreamDef(field='label', shape=NUM_CLASSES)  # and second as 'label'\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise model and trainer\n",
    "We need to normalise the values in the image as well and initialise the convolution network we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_model(input_var, model_func):\n",
    "    # Normalize the input\n",
    "    feature_scale = 1.0 / 256.0\n",
    "    input_var_norm = element_times(feature_scale, input_var)\n",
    "\n",
    "    # apply model to input\n",
    "    initialised_model = model_func(input_var_norm, out_dims=10)\n",
    "    log_number_of_parameters(initialised_model)\n",
    "    return initialised_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the the optimiser used to train the neural network. We are using Adam since it compares favourably with other optimisation techniques [[1]](https://arxiv.org/abs/1412.6980). I found it hard to reconcile the naming of the variables in the [paper](https://arxiv.org/abs/1412.6980) to those of the function. Thankfully I found the [answer here](http://stackoverflow.com/questions/41305918/in-cntk-implementation-of-adam-optimizer-how-the-parameters-alpha-beta1-beta2/41305959#41305959)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_trainer(initialised_model, label_var, minibatch_size=64, epoch_size=50000):\n",
    "    # loss and metric\n",
    "    ce = cross_entropy_with_softmax(initialised_model, label_var)\n",
    "    pe = classification_error(initialised_model, label_var)\n",
    "\n",
    "    # Set training parameters\n",
    "    lr_per_minibatch = learning_rate_schedule(0.005, UnitType.minibatch)\n",
    "    beta1 = momentum_schedule(0.9)\n",
    "    beta2 = momentum_schedule(0.999)\n",
    "    # Adam optimiser\n",
    "    learner = adam_sgd(initialised_model.parameters,\n",
    "                       lr_per_minibatch,\n",
    "                       beta1,\n",
    "                       variance_momentum=beta2)\n",
    "    \n",
    "    trainer = Trainer(initialised_model, ce, pe, [learner])\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_size_iterator_from(epoch_size, batch_size):\n",
    "    ''' Returns iterator of batch sizes\n",
    "    '''\n",
    "    complete_batch_count, remainder = epoch_size//batch_size, epoch_size%batch_size\n",
    "    remainder_list = [remainder] if remainder>0 else []\n",
    "    return chain(repeat(batch_size, complete_batch_count),remainder_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function below to generate batches of data which we can then use to train the network with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch_generator(data_reader, input_map, minibatch_size=64, epoch_size=50000):\n",
    "    ''' Generates batches of data\n",
    "    '''\n",
    "    for batch_size in batch_size_iterator_from(epoch_size, minibatch_size):\n",
    "        yield data_reader.next_minibatch(batch_size, input_map=input_map)  # fetch minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_map_for(data_reader, input_var, label_var):\n",
    "    return {\n",
    "        input_var: data_reader.streams.features,\n",
    "        label_var: data_reader.streams.labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Below we train the network. We can specify how long we wish to train the network for by changing the value of the variable max_epochs. We will simply let the network run for 20 epochs. Which means we will run through the training data 20 times. \n",
    "\n",
    "In the code below we can see that we have an outer loop that runs over the number of epochs and an inner loop that runs over the minibatches in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 145578 parameters in 10 parameter tensors.\n",
      "Finished Epoch[1 of 300]: [Training] loss = 1.809947 * 50000, metric = 66.7% * 50000 109.960s (454.7 samples per second)\n",
      "Finished Epoch[2 of 300]: [Training] loss = 1.425233 * 50000, metric = 51.6% * 50000 110.140s (454.0 samples per second)\n",
      "Finished Epoch[3 of 300]: [Training] loss = 1.283573 * 50000, metric = 46.2% * 50000 109.582s (456.3 samples per second)\n",
      "Finished Epoch[4 of 300]: [Training] loss = 1.168716 * 50000, metric = 41.8% * 50000 109.393s (457.1 samples per second)\n",
      "Finished Epoch[5 of 300]: [Training] loss = 1.071775 * 50000, metric = 38.2% * 50000 109.925s (454.9 samples per second)\n",
      "Finished Epoch[6 of 300]: [Training] loss = 0.993943 * 50000, metric = 35.4% * 50000 109.230s (457.8 samples per second)\n",
      "Finished Epoch[7 of 300]: [Training] loss = 0.936975 * 50000, metric = 33.1% * 50000 109.148s (458.1 samples per second)\n",
      "Finished Epoch[8 of 300]: [Training] loss = 0.887111 * 50000, metric = 31.2% * 50000 108.306s (461.7 samples per second)\n",
      "Finished Epoch[9 of 300]: [Training] loss = 0.842387 * 50000, metric = 29.7% * 50000 108.884s (459.2 samples per second)\n",
      "Finished Epoch[10 of 300]: [Training] loss = 0.807293 * 50000, metric = 28.5% * 50000 110.588s (452.1 samples per second)\n",
      "Finished Epoch[11 of 300]: [Training] loss = 0.774349 * 50000, metric = 27.2% * 50000 109.669s (455.9 samples per second)\n",
      "Finished Epoch[12 of 300]: [Training] loss = 0.744370 * 50000, metric = 26.0% * 50000 110.087s (454.2 samples per second)\n",
      "Finished Epoch[13 of 300]: [Training] loss = 0.722810 * 50000, metric = 25.4% * 50000 108.575s (460.5 samples per second)\n",
      "Finished Epoch[14 of 300]: [Training] loss = 0.693186 * 50000, metric = 24.3% * 50000 109.594s (456.2 samples per second)\n",
      "Finished Epoch[15 of 300]: [Training] loss = 0.669935 * 50000, metric = 23.5% * 50000 109.135s (458.2 samples per second)\n",
      "Finished Epoch[16 of 300]: [Training] loss = 0.647154 * 50000, metric = 22.7% * 50000 109.393s (457.1 samples per second)\n",
      "Finished Epoch[17 of 300]: [Training] loss = 0.634166 * 50000, metric = 22.3% * 50000 109.318s (457.4 samples per second)\n",
      "Finished Epoch[18 of 300]: [Training] loss = 0.611851 * 50000, metric = 21.4% * 50000 108.846s (459.4 samples per second)\n",
      "Finished Epoch[19 of 300]: [Training] loss = 0.592638 * 50000, metric = 20.8% * 50000 108.870s (459.3 samples per second)\n",
      "Finished Epoch[20 of 300]: [Training] loss = 0.572997 * 50000, metric = 20.3% * 50000 109.573s (456.3 samples per second)\n",
      "CPU times: user 4h 20min 59s, sys: 5min 20s, total: 4h 26min 19s\n",
      "Wall time: 36min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Input variables denoting the features and label data\n",
    "input_var = input_variable((NUM_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "label_var = input_variable((NUM_CLASSES))\n",
    "data_path = os.path.join('data', 'CIFAR-10')\n",
    "\n",
    "reader_train = create_reader(os.path.join(data_path, 'train_map.txt'),\n",
    "                             os.path.join(data_path, 'CIFAR-10_mean.xml'),\n",
    "                             True)\n",
    "\n",
    "progress_printer = ProgressPrinter(tag='Training')\n",
    "initialised_model = initialise_model(input_var, create_model)\n",
    "trainer = initialise_trainer(initialised_model, label_var)\n",
    "input_map = input_map_for(reader_train, input_var, label_var)\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in range(max_epochs): # Epoch loop\n",
    "    for batch_data in minibatch_generator(reader_train, input_map, minibatch_size=64, epoch_size=50000):# Minibatch\n",
    "        trainer.train_minibatch(batch_data)\n",
    "        progress_printer.update_with_trainer(trainer, with_metric=True) # log progress\n",
    "    progress_printer.epoch_summary(with_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Once the network is trained we can simply run the test data against it and see how well our nework does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test error rate: 0.507\n"
     ]
    }
   ],
   "source": [
    "reader_test = create_reader(os.path.join(data_path, 'test_map.txt'),\n",
    "                            os.path.join(data_path, 'CIFAR-10_mean.xml'),\n",
    "                            False)\n",
    "epoch_size = 10000\n",
    "minibatch_size = 16\n",
    "batch_gen = minibatch_generator(reader_test, input_map, minibatch_size=minibatch_size, epoch_size=epoch_size)\n",
    "metric_per_batch = [trainer.test_minibatch(batch_data) for batch_data in batch_gen]\n",
    "batch_weight = list(batch_size_iterator_from(epoch_size, minibatch_size))\n",
    "print(\"Average test error rate: {}\".format(np.average(metric_per_batch, weights=batch_weight)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The error rate we get is around 33%. You can train the network for longer, use different training parameters or even change the structure of the network."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
