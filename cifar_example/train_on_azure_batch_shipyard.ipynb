{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Deep Learning Model on Azure Batch Shipyard\n",
    "This notebook is a simple example on how to train a Deep Learning Model on GPUs using Docker containers and Azure Batch Shipyard. The whole thing can be mostly orchestrated through a Jupyter notebook which makes it very easy to intermix commands, create JSON teamplates and see the output of the executed commands. The reason for doing so is mainly pedagogical and not meant to be the recommended way of doing this. We will be using the Microsoft DSVM which comes with many tools already installed such as Anaconda, Docker, Azure CLI and Blobxfer.\n",
    "\n",
    "Most of the instructions apply to other Linux distros, just be mindful of the distro specific requirements.\n",
    "Tools you will need to install if you are not using a Microsoft DSVM\n",
    "* Azure Cli: Command line tool to manage azure resources. See [here](https://github.com/Azure/azure-cli) for installation instructions\n",
    "* blobxfer: A file transfer tool. See [here](https://github.com/Azure/blobxfer) for installation instructions\n",
    "* Docker: [See here](https://docs.docker.com/engine/installation/)\n",
    "* Anaconda: [See here](https://docs.continuum.io/anaconda/install)\n",
    "\n",
    "<span style=\"color:red\">You need an Azure subscription with access to GPU enabled VMs to run this example</span>\n",
    "\n",
    "\n",
    "** Many of the variables defined in the notebook can be called whatever you want. The only one that you need to be mindful of is the Azure subscription you wish to use. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Docker\n",
    "<span style=\"color:blue\">[If you have Docker already setup and able to execute it without invoking sudo you can ignore these instructions]</span>\n",
    "\n",
    "The Docker engine comes ready installed on the Microsoft DSVM. If you need to install Docker look at [these instructions](https://docs.docker.com/engine/installation/). \n",
    "\n",
    "As it is set up we need to still use sudo to invoke Docker which we can not do from within the Jupyter notebook. Therefore we do the following. (Instructions taken from [here](https://docs.docker.com/engine/installation/linux/centos/))\n",
    "\n",
    "##### Create a Docker group\n",
    "\n",
    "The Docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can access it with sudo. For this reason, Docker daemon always runs as the root user.\n",
    "\n",
    "To avoid having to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read/writable by the docker group.\n",
    "\n",
    "To create the docker group and add your user:\n",
    "\n",
    "Log into your machine as a user with sudo or root privileges.\n",
    "\n",
    "Create the docker group.\n",
    "    \n",
    "```bash\n",
    "sudo groupadd docker\n",
    "```\n",
    "\n",
    "Add your user to docker group.\n",
    "\n",
    "```bash\n",
    "sudo usermod -aG docker your_username\n",
    "```\n",
    "\n",
    "Log out and log back in.\n",
    "This ensures your user is running with the correct permissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your user is in the docker group by running docker without sudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Hello from Docker!\r\n",
      "This message shows that your installation appears to be working correctly.\r\n",
      "\r\n",
      "To generate this message, Docker took the following steps:\r\n",
      " 1. The Docker client contacted the Docker daemon.\r\n",
      " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\r\n",
      " 3. The Docker daemon created a new container from that image which runs the\r\n",
      "    executable that produces the output you are currently reading.\r\n",
      " 4. The Docker daemon streamed that output to the Docker client, which sent it\r\n",
      "    to your terminal.\r\n",
      "\r\n",
      "To try something more ambitious, you can run an Ubuntu container with:\r\n",
      " $ docker run -it ubuntu bash\r\n",
      "\r\n",
      "Share images, automate workflows, and more with a free Docker ID:\r\n",
      " https://cloud.docker.com/\r\n",
      "\r\n",
      "For more examples and ideas, visit:\r\n",
      " https://docs.docker.com/engine/userguide/\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm hello-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model\n",
    "The model we will be using is a simple convolution network which we will train and evaluate on the CIFAR 10 dataset.\n",
    "\n",
    "The two notebooks are [Process CIFAR data.ipynb](process_cifar_data.ipynb) and [CNTK CIFAR10](cntk_cifar10.ipynb)\n",
    "\n",
    "We first create the appropriate directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘script’: File exists\n",
      "mkdir: cannot create directory ‘script/code’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir script\n",
    "!mkdir script/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we copy the notebooks to the appropriate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp process_cifar_data.ipynb script/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp cntk_cifar10.ipynb script/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once copied we convert them to Python files. This isn't necessary since we could run them as notebooks but requires some extra configuration steps which we can avoid by converting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script/code/process_cifar_data.ipynb to python\n",
      "[NbConvertApp] Writing 6344 bytes to script/code/process_cifar_data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python script/code/process_cifar_data.ipynb --ExecutePreprocessor.kernel_name=cntk-py34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script/code/cntk_cifar10.ipynb to python\n",
      "[NbConvertApp] Writing 7925 bytes to script/code/cntk_cifar10.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python script/code/cntk_cifar10.ipynb --ExecutePreprocessor.kernel_name=cntk-py34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login and configure Azure CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">OUTPUT CLEARED FOR CONFIDENTIALITY REASONS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to do the following if you have more than one Azure subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_subscription = \"'My Subscription'\" # ADD THE NAME OR ID OF THE SUBSCRIPTION YOU WANT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!az account set --subscription $selected_subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Azure subscriptions and check we have selected the right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!az account list -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">OUTPUT CLEARED FOR CONFIDENTIALITY REASONS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the private docker registry and collect the necessary information. For this example we will call the registry and group as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docker_registry = \"mscontainer\"\n",
    "docker_registry_group = \"mscontainergorup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location        Name\r\n",
      "--------------  ----------------\r\n",
      "southcentralus  mscontainergorup\r\n"
     ]
    }
   ],
   "source": [
    "!az group create -n $docker_registry_group -l southcentralus -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\r\n",
      "Create a new service principal and assign access:\u001b[0m\r\n",
      "\u001b[33m  az ad sp create-for-rbac --scopes /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/mscontainergorup/providers/Microsoft.ContainerRegistry/registries/mscontainer --role Owner --password <password>\u001b[0m\r\n",
      "\u001b[33m\r\n",
      "Use an existing service principal and assign access:\u001b[0m\r\n",
      "\u001b[33m  az role assignment create --scope /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/mscontainergorup/providers/Microsoft.ContainerRegistry/registries/mscontainer --role Owner --assignee <app-id>\u001b[0m\r\n",
      "NAME         RESOURCE GROUP    LOCATION        LOGIN SERVER                      CREATION DATE                     ADMIN ENABLED\r\n",
      "-----------  ----------------  --------------  --------------------------------  --------------------------------  ---------------\r\n",
      "mscontainer  mscontainergorup  southcentralus  mscontainer-microsoft.azurecr.io  2017-02-23T13:01:48.164277+00:00  False\r\n"
     ]
    }
   ],
   "source": [
    "!az acr create -n $docker_registry -g $docker_registry_group -l southcentralus -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         RESOURCE GROUP    LOCATION        LOGIN SERVER                      CREATION DATE                     ADMIN ENABLED\r\n",
      "-----------  ----------------  --------------  --------------------------------  --------------------------------  ---------------\r\n",
      "mscontainer  mscontainergorup  southcentralus  mscontainer-microsoft.azurecr.io  2017-02-23T13:01:48.164277+00:00  True\r\n"
     ]
    }
   ],
   "source": [
    "!az acr update -n $docker_registry --admin-enabled true -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az acr credential show -n $docker_registry\n",
    "docker_username = json.loads(''.join(json_data))['username']\n",
    "docker_password = json.loads(''.join(json_data))['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az acr show -n $docker_registry\n",
    "docker_registry_server = json.loads(''.join(json_data))['loginServer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘script/docker’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir script/docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the dockerfile\n",
    "The dockerfile defines what we want in our Docker image. CNTK has some prebuilt images which we can use as a base to build upon. The image has everything installed so we don't have to worry about installing GPU drivers etc. We simply add the code directory we created earlier which contains our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile script/docker/dockerfile\n",
    "# Dockerfile for CNTK-GPU-OpenMPI for use with Batch Shipyard on Azure Batch\n",
    "\n",
    "FROM microsoft/cntk:2.0.beta8.0-runtime-gpu-python3.4-cuda8.0-cudnn5.1\n",
    "MAINTAINER Mathew Salvaris\n",
    "ADD code /code\n",
    "ENV PATH /root/anaconda3/bin:$PATH\n",
    "CMD [ \"/bin/bash\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "container_name = docker_registry_server+\"/masalvar/cntkbatch\"\n",
    "application_path = 'script'\n",
    "docker_file_location = path.join(application_path, 'docker/dockerfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\r\n"
     ]
    }
   ],
   "source": [
    "!docker login $docker_registry_server -u $docker_username -p $docker_password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Docker Image\n",
    "It can take some time to pull the CNTK image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon 45.57 kB\n",
      "Step 1 : FROM microsoft/cntk:2.0.beta8.0-runtime-gpu-python3.4-cuda8.0-cudnn5.1\n",
      " ---> c5b08f2fba7b\n",
      "Step 2 : MAINTAINER Mathew Salvaris <mathew.salvaris@microsoft.com>\n",
      " ---> Running in d4442f9f2dc3\n",
      " ---> ccecae2d8314\n",
      "Removing intermediate container d4442f9f2dc3\n",
      "Step 3 : ADD code /code\n",
      " ---> 3e73e652b70a\n",
      "Removing intermediate container ce2395e793df\n",
      "Step 4 : ENV PATH /root/anaconda3/bin:$PATH\n",
      " ---> Running in 0e1c677a2ff8\n",
      " ---> f19dd8675760\n",
      "Removing intermediate container 0e1c677a2ff8\n",
      "Step 5 : CMD /bin/bash\n",
      " ---> Running in e8c1f3277392\n",
      " ---> 17b08a178ef7\n",
      "Removing intermediate container e8c1f3277392\n",
      "Successfully built 17b08a178ef7\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $container_name -f $docker_file_location $application_path --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker push $container_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">OUTPUT CLEARED FOR CONFIDENTIALITY REASONS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Batch Shipyard\n",
    "<span style=\"color:blue\">[If you have Batch Shipyard installed somewhere you can ignore these instructions]</span>\n",
    "\n",
    "[Based on these instructions](https://github.com/Azure/batch-shipyard/blob/master/docs/01-batch-shipyard-installation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'batch-shipyard'...\n",
      "remote: Counting objects: 2957, done.\u001b[K\n",
      "remote: Total 2957 (delta 0), reused 0 (delta 0), pack-reused 2956\u001b[K\n",
      "Receiving objects: 100% (2957/2957), 978.75 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (1998/1998), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Azure/batch-shipyard.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can not be done from inside the notebook since it requires root privilidges so execute the following in the terminal\n",
    "```bash\n",
    "cd batch-shipyard\n",
    "pip install -r requirements.txt\n",
    "./install.sh -3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -3 switch is to install batch-shipyard for python 3 which is what is recommeded and what we will do.\n",
    "\n",
    "Create a reference to the scripts. Instead of doing this you could add it to your path or create a symlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchshipyard = 'batch-shipyard/shipyard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Shipyard\n",
    "In order to use Batch Shipyard we need to prepare a number of configuration json files. [Look here for more details](https://github.com/Azure/batch-shipyard/blob/master/docs/10-batch-shipyard-configuration.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Account and Batch Storage\n",
    "Batch shipyard requires a storage account for storing metadata in order to execute across a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_name = 'msbatchexample'\n",
    "location = 'southcentralus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location        Name\n",
      "--------------  --------------\n",
      "southcentralus  msbatchexample\n",
      "CPU times: user 59.9 ms, sys: 27.2 ms, total: 87.1 ms\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az group create -n $group_name -l $location -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!az group list -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">OUTPUT CLEARED FOR CONFIDENTIALITY REASONS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the batch account and the storage account which we associate with the group we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_account_name = \"msbatchex\"\n",
    "storage_account_name = \"msbatchstoreex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM template to create batch account and batch storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"batchAccounts_name\": {\n",
    "            \"defaultValue\": batch_account_name,\n",
    "            \"type\": \"String\"\n",
    "        },\n",
    "        \"storageAccounts_name\": {\n",
    "            \"defaultValue\": storage_account_name,\n",
    "            \"type\": \"String\"\n",
    "        }\n",
    "    },\n",
    "    \"variables\": {},\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"type\": \"Microsoft.Batch/batchAccounts\",\n",
    "            \"name\": \"[parameters('batchAccounts_name')]\",\n",
    "            \"apiVersion\": \"2015-12-01\",\n",
    "            \"location\": location,\n",
    "            \"properties\": {\n",
    "                \"autoStorage\": {\n",
    "                    \"storageAccountId\": \"[resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccounts_name'))]\"\n",
    "                }\n",
    "            },\n",
    "            \"resources\": [],\n",
    "            \"dependsOn\": [\n",
    "                \"[resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccounts_name'))]\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Microsoft.Storage/storageAccounts\",\n",
    "            \"sku\": {\n",
    "                \"name\": \"Standard_LRS\",\n",
    "                \"tier\": \"Standard\"\n",
    "            },\n",
    "            \"kind\": \"Storage\",\n",
    "            \"name\": \"[parameters('storageAccounts_name')]\",\n",
    "            \"apiVersion\": \"2016-01-01\",\n",
    "            \"location\": location,\n",
    "            \"tags\": {},\n",
    "            \"properties\": {},\n",
    "            \"resources\": [],\n",
    "            \"dependsOn\": []\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_filename = 'template.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(template_filename, 'w') as outfile:\n",
    "    json.dump(template_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"error\": null,\r\n",
      "  \"properties\": {\r\n",
      "    \"correlationId\": \"60de92eb-a595-4016-b14d-d5a008cae09c\",\r\n",
      "    \"debugSetting\": null,\r\n",
      "    \"dependencies\": [\r\n",
      "      {\r\n",
      "        \"dependsOn\": [\r\n",
      "          {\r\n",
      "            \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Storage/storageAccounts/msbatchstoreex\",\r\n",
      "            \"resourceGroup\": \"msbatchexample\",\r\n",
      "            \"resourceName\": \"msbatchstoreex\",\r\n",
      "            \"resourceType\": \"Microsoft.Storage/storageAccounts\"\r\n",
      "          }\r\n",
      "        ],\r\n",
      "        \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Batch/batchAccounts/msbatchex\",\r\n",
      "        \"resourceGroup\": \"msbatchexample\",\r\n",
      "        \"resourceName\": \"msbatchex\",\r\n",
      "        \"resourceType\": \"Microsoft.Batch/batchAccounts\"\r\n",
      "      }\r\n",
      "    ],\r\n",
      "    \"mode\": \"Incremental\",\r\n",
      "    \"outputs\": null,\r\n",
      "    \"parameters\": {\r\n",
      "      \"batchAccounts_name\": {\r\n",
      "        \"type\": \"String\",\r\n",
      "        \"value\": \"msbatchex\"\r\n",
      "      },\r\n",
      "      \"storageAccounts_name\": {\r\n",
      "        \"type\": \"String\",\r\n",
      "        \"value\": \"msbatchstoreex\"\r\n",
      "      }\r\n",
      "    },\r\n",
      "    \"parametersLink\": null,\r\n",
      "    \"providers\": [\r\n",
      "      {\r\n",
      "        \"id\": null,\r\n",
      "        \"namespace\": \"Microsoft.Batch\",\r\n",
      "        \"registrationState\": null,\r\n",
      "        \"resourceTypes\": [\r\n",
      "          {\r\n",
      "            \"aliases\": null,\r\n",
      "            \"apiVersions\": null,\r\n",
      "            \"locations\": [\r\n",
      "              \"southcentralus\"\r\n",
      "            ],\r\n",
      "            \"properties\": null,\r\n",
      "            \"resourceType\": \"batchAccounts\"\r\n",
      "          }\r\n",
      "        ]\r\n",
      "      },\r\n",
      "      {\r\n",
      "        \"id\": null,\r\n",
      "        \"namespace\": \"Microsoft.Storage\",\r\n",
      "        \"registrationState\": null,\r\n",
      "        \"resourceTypes\": [\r\n",
      "          {\r\n",
      "            \"aliases\": null,\r\n",
      "            \"apiVersions\": null,\r\n",
      "            \"locations\": [\r\n",
      "              \"southcentralus\"\r\n",
      "            ],\r\n",
      "            \"properties\": null,\r\n",
      "            \"resourceType\": \"storageAccounts\"\r\n",
      "          }\r\n",
      "        ]\r\n",
      "      }\r\n",
      "    ],\r\n",
      "    \"provisioningState\": \"Accepted\",\r\n",
      "    \"template\": null,\r\n",
      "    \"templateLink\": null,\r\n",
      "    \"timestamp\": \"2017-02-23T13:15:05.797677+00:00\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az group deployment validate --template-file $template_filename -g $group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarting long running operation 'Starting group deployment create'\u001b[0m\n",
      "\u001b[32mLong running operation 'Starting group deployment create' completed with result {'id': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Resources/deployments/template', 'name': 'template', 'properties': <azure.mgmt.resource.resources.models.deployment_properties_extended.DeploymentPropertiesExtended object at 0x7fb411618668>}\u001b[0m\n",
      "{\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Resources/deployments/template\",\n",
      "  \"name\": \"template\",\n",
      "  \"properties\": {\n",
      "    \"correlationId\": \"46dbfa1f-1578-4534-acf0-f67a6376b323\",\n",
      "    \"debugSetting\": null,\n",
      "    \"dependencies\": [\n",
      "      {\n",
      "        \"dependsOn\": [\n",
      "          {\n",
      "            \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Storage/storageAccounts/msbatchstoreex\",\n",
      "            \"resourceGroup\": \"msbatchexample\",\n",
      "            \"resourceName\": \"msbatchstoreex\",\n",
      "            \"resourceType\": \"Microsoft.Storage/storageAccounts\"\n",
      "          }\n",
      "        ],\n",
      "        \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/msbatchexample/providers/Microsoft.Batch/batchAccounts/msbatchex\",\n",
      "        \"resourceGroup\": \"msbatchexample\",\n",
      "        \"resourceName\": \"msbatchex\",\n",
      "        \"resourceType\": \"Microsoft.Batch/batchAccounts\"\n",
      "      }\n",
      "    ],\n",
      "    \"mode\": \"Incremental\",\n",
      "    \"outputs\": null,\n",
      "    \"parameters\": {\n",
      "      \"batchAccounts_name\": {\n",
      "        \"type\": \"String\",\n",
      "        \"value\": \"msbatchex\"\n",
      "      },\n",
      "      \"storageAccounts_name\": {\n",
      "        \"type\": \"String\",\n",
      "        \"value\": \"msbatchstoreex\"\n",
      "      }\n",
      "    },\n",
      "    \"parametersLink\": null,\n",
      "    \"providers\": [\n",
      "      {\n",
      "        \"id\": null,\n",
      "        \"namespace\": \"Microsoft.Batch\",\n",
      "        \"registrationState\": null,\n",
      "        \"resourceTypes\": [\n",
      "          {\n",
      "            \"aliases\": null,\n",
      "            \"apiVersions\": null,\n",
      "            \"locations\": [\n",
      "              \"southcentralus\"\n",
      "            ],\n",
      "            \"properties\": null,\n",
      "            \"resourceType\": \"batchAccounts\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"id\": null,\n",
      "        \"namespace\": \"Microsoft.Storage\",\n",
      "        \"registrationState\": null,\n",
      "        \"resourceTypes\": [\n",
      "          {\n",
      "            \"aliases\": null,\n",
      "            \"apiVersions\": null,\n",
      "            \"locations\": [\n",
      "              \"southcentralus\"\n",
      "            ],\n",
      "            \"properties\": null,\n",
      "            \"resourceType\": \"storageAccounts\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"provisioningState\": \"Succeeded\",\n",
      "    \"template\": null,\n",
      "    \"templateLink\": null,\n",
      "    \"timestamp\": \"2017-02-23T13:16:12.848565+00:00\"\n",
      "  },\n",
      "  \"resourceGroup\": \"msbatchexample\"\n",
      "}\n",
      "CPU times: user 2.23 s, sys: 606 ms, total: 2.84 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az group deployment create --template-file $template_filename -g $group_name --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gather batch account info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account keys list -n $batch_account_name -g $group_name\n",
    "batch_account_key = json.loads(''.join(json_data))['primary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account list -g $group_name\n",
    "batch_service_url = 'https://'+json.loads(''.join(json_data))[0]['accountEndpoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get storage account key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $storage_account_name -g $group_name\n",
    "storage_account_key = json.loads(''.join(json_data))[0]['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storage_alias = \"mystorageaccount\"\n",
    "storage_endpoint = \"core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"credentials\": {\n",
    "        \"batch\": {\n",
    "            \"account\": batch_account_name,\n",
    "            \"account_key\": batch_account_key,\n",
    "            \"account_service_url\": batch_service_url\n",
    "        },\n",
    "        \"storage\": {\n",
    "            storage_alias : {\n",
    "                    \"account\": storage_account_name,\n",
    "                    \"account_key\": storage_account_key,\n",
    "                    \"endpoint\": storage_endpoint\n",
    "            }\n",
    "        },\n",
    "        \"docker_registry\": {\n",
    "            docker_registry_server : {\n",
    "                    \"username\": docker_username,\n",
    "                    \"password\": docker_password\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": storage_alias\n",
    "    },\n",
    "    \"docker_registry\": {\n",
    "        \"private\": {\n",
    "            \"allow_public_docker_hub_pull_on_missing\": True,\n",
    "            \"server\": docker_registry_server\n",
    "        }\n",
    "    },\n",
    "    \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            container_name\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pool configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool={\n",
    "    \"pool_specification\": {\n",
    "        \"id\": \"scikit\",\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": 1,\n",
    "        \"publisher\": \"Canonical\",\n",
    "        \"offer\": \"UbuntuServer\",\n",
    "        \"sku\": \"16.04.0-LTS\",\n",
    "        \"ssh\": {\n",
    "            \"username\": \"docker\"\n",
    "        },\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jobs configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": \"cntkjob\",\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": \"run_cifar\",# This should be changed per task\n",
    "                    \"image\": container_name,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": 'bash -c \"source /cntk/activate-cntk;python /code/process_cifar_data.py;ipython /code/cntk_cifar10.py\"',\n",
    "                    \"gpu\": True,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘config’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_json_to_file(json_dict, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_json_to_file(credentials, path.join('config', 'credentials.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(config, path.join('config', 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(pool, path.join('config', 'pool.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(jobs, path.join('config', 'jobs.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execture Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pool based on the configuration files we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 2 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%bash --bg --proc pool_proc\n",
    "batch-shipyard/shipyard pool add --yes --configdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a bit before adding the jobs to give a the VM a chance to spin up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check status of the pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 19:18:44,345Z INFO convoy.batch:list_pools:556 pool_id=scikit [state=PoolState.active allocation_state=AllocationState.steady vm_size=standard_nc6, vm_count=1 target_vm_count=1]\r\n"
     ]
    }
   ],
   "source": [
    "!$batchshipyard pool list --configdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the job to the queue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 19:37:00,192Z INFO convoy.batch:add_jobs:1722 Adding job: cntkjob\n",
      "2017-02-23 19:37:00,545Z DEBUG convoy.storage:upload_resource_files:333 remote file is the same for shipyardtaskrf-cntkjob/run_cifar.shipyard.envlist, skipping\n",
      "2017-02-23 19:37:00,546Z INFO convoy.batch:add_jobs:1921 Adding task: run_cifar\n",
      "2017-02-23 19:37:00,621Z DEBUG convoy.batch:stream_file_and_wait_for_task:1192 attempting to stream file stdout.txt from job=cntkjob task=run_cifar\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Done.\n",
      "Extracting files...\n",
      "Done.\n",
      "Preparing train set...\n",
      "Done.\n",
      "Preparing test set...\n",
      "Done.\n",
      "Writing train text file...\n",
      "Done.\n",
      "Writing test text file...\n",
      "Done.\n",
      "Converting train data to png images...\n",
      "Done.\n",
      "Converting test data to png images...\n",
      "Done.\n",
      "Training 145578 parameters in 10 parameter tensors.\n",
      "Finished Epoch[1 of 300]: [Training] loss = 1.789220 * 50000, metric = 65.6% * 50000 7.543s (6628.3 samples per second)\n",
      "Finished Epoch[2 of 300]: [Training] loss = 1.439335 * 50000, metric = 52.0% * 50000 6.628s (7544.0 samples per second)\n",
      "Finished Epoch[3 of 300]: [Training] loss = 1.290928 * 50000, metric = 46.7% * 50000 7.262s (6884.9 samples per second)\n",
      "Finished Epoch[4 of 300]: [Training] loss = 1.170365 * 50000, metric = 41.6% * 50000 6.661s (7505.9 samples per second)\n",
      "Finished Epoch[5 of 300]: [Training] loss = 1.074584 * 50000, metric = 38.1% * 50000 7.241s (6905.3 samples per second)\n",
      "Finished Epoch[6 of 300]: [Training] loss = 0.999360 * 50000, metric = 35.5% * 50000 6.604s (7570.8 samples per second)\n",
      "Finished Epoch[7 of 300]: [Training] loss = 0.943127 * 50000, metric = 33.3% * 50000 7.292s (6856.4 samples per second)\n",
      "Finished Epoch[8 of 300]: [Training] loss = 0.895473 * 50000, metric = 31.6% * 50000 6.762s (7394.5 samples per second)\n",
      "Finished Epoch[9 of 300]: [Training] loss = 0.851860 * 50000, metric = 30.2% * 50000 7.097s (7045.1 samples per second)\n",
      "Finished Epoch[10 of 300]: [Training] loss = 0.816719 * 50000, metric = 28.8% * 50000 7.240s (6905.8 samples per second)\n",
      "Finished Epoch[11 of 300]: [Training] loss = 0.785226 * 50000, metric = 27.6% * 50000 7.726s (6471.5 samples per second)\n",
      "Finished Epoch[12 of 300]: [Training] loss = 0.757090 * 50000, metric = 26.6% * 50000 7.176s (6967.8 samples per second)\n",
      "Finished Epoch[13 of 300]: [Training] loss = 0.736490 * 50000, metric = 25.9% * 50000 7.551s (6621.9 samples per second)\n",
      "Finished Epoch[14 of 300]: [Training] loss = 0.710469 * 50000, metric = 24.9% * 50000 7.061s (7081.0 samples per second)\n",
      "Finished Epoch[15 of 300]: [Training] loss = 0.686282 * 50000, metric = 24.2% * 50000 7.700s (6493.7 samples per second)\n",
      "Finished Epoch[16 of 300]: [Training] loss = 0.665058 * 50000, metric = 23.3% * 50000 7.426s (6733.4 samples per second)\n",
      "Finished Epoch[17 of 300]: [Training] loss = 0.650206 * 50000, metric = 22.9% * 50000 7.791s (6417.5 samples per second)\n",
      "Finished Epoch[18 of 300]: [Training] loss = 0.630838 * 50000, metric = 22.1% * 50000 7.596s (6582.3 samples per second)\n",
      "Finished Epoch[19 of 300]: [Training] loss = 0.608409 * 50000, metric = 21.6% * 50000 8.019s (6235.3 samples per second)\n",
      "Finished Epoch[20 of 300]: [Training] loss = 0.592393 * 50000, metric = 21.0% * 50000 7.417s (6741.3 samples per second)\n",
      "CPU times: user 2min 59s, sys: 3min 59s, total: 6min 59s\n",
      "Wall time: 2min 27s\n",
      "Average test error rate: 0.3282\n",
      "\n",
      "CPU times: user 10.2 s, sys: 2.86 s, total: 13 s\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!$batchshipyard jobs add --configdir config --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stream stdout and stderr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 14:08:39,292Z DEBUG convoy.batch:stream_file_and_wait_for_task:1192 attempting to stream file stdout.txt from job=cntkjob task=run_cifar\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Done.\n",
      "Extracting files...\n",
      "Done.\n",
      "Preparing train set...\n",
      "Done.\n",
      "Preparing test set...\n",
      "Done.\n",
      "Writing train text file...\n",
      "Done.\n",
      "Writing test text file...\n",
      "Done.\n",
      "Converting train data to png images...\n",
      "Done.\n",
      "Converting test data to png images...\n",
      "Done.\n",
      "Training 145578 parameters in 10 parameter tensors.\n",
      "Finished Epoch[1 of 300]: [Training] loss = 1.789279 * 50000, metric = 65.5% * 50000 6.931s (7213.8 samples per second)\n",
      "Finished Epoch[2 of 300]: [Training] loss = 1.439543 * 50000, metric = 52.1% * 50000 7.203s (6941.5 samples per second)\n",
      "Finished Epoch[3 of 300]: [Training] loss = 1.291373 * 50000, metric = 46.7% * 50000 6.702s (7460.4 samples per second)\n",
      "Finished Epoch[4 of 300]: [Training] loss = 1.170808 * 50000, metric = 41.7% * 50000 7.096s (7046.3 samples per second)\n",
      "Finished Epoch[5 of 300]: [Training] loss = 1.075077 * 50000, metric = 38.1% * 50000 6.804s (7348.9 samples per second)\n",
      "Finished Epoch[6 of 300]: [Training] loss = 0.999707 * 50000, metric = 35.5% * 50000 7.272s (6875.9 samples per second)\n",
      "Finished Epoch[7 of 300]: [Training] loss = 0.943416 * 50000, metric = 33.2% * 50000 6.969s (7175.0 samples per second)\n",
      "Finished Epoch[8 of 300]: [Training] loss = 0.895959 * 50000, metric = 31.5% * 50000 7.554s (6619.1 samples per second)\n",
      "Finished Epoch[9 of 300]: [Training] loss = 0.851782 * 50000, metric = 30.1% * 50000 7.264s (6883.4 samples per second)\n",
      "Finished Epoch[10 of 300]: [Training] loss = 0.817051 * 50000, metric = 28.9% * 50000 7.394s (6762.0 samples per second)\n",
      "Finished Epoch[11 of 300]: [Training] loss = 0.785344 * 50000, metric = 27.6% * 50000 7.459s (6703.3 samples per second)\n",
      "Finished Epoch[12 of 300]: [Training] loss = 0.757034 * 50000, metric = 26.6% * 50000 7.701s (6492.5 samples per second)\n",
      "Finished Epoch[13 of 300]: [Training] loss = 0.736727 * 50000, metric = 26.0% * 50000 7.447s (6714.1 samples per second)\n",
      "Finished Epoch[14 of 300]: [Training] loss = 0.710788 * 50000, metric = 24.9% * 50000 7.679s (6511.6 samples per second)\n",
      "Finished Epoch[15 of 300]: [Training] loss = 0.687136 * 50000, metric = 24.3% * 50000 7.370s (6784.3 samples per second)\n",
      "Finished Epoch[16 of 300]: [Training] loss = 0.665321 * 50000, metric = 23.4% * 50000 7.809s (6402.7 samples per second)\n",
      "Finished Epoch[17 of 300]: [Training] loss = 0.650228 * 50000, metric = 22.9% * 50000 7.885s (6341.4 samples per second)\n",
      "Finished Epoch[18 of 300]: [Training] loss = 0.630876 * 50000, metric = 22.1% * 50000 7.983s (6263.7 samples per second)\n",
      "Finished Epoch[19 of 300]: [Training] loss = 0.608805 * 50000, metric = 21.5% * 50000 7.873s (6351.0 samples per second)\n",
      "Finished Epoch[20 of 300]: [Training] loss = 0.593298 * 50000, metric = 20.8% * 50000 8.152s (6133.1 samples per second)\n",
      "CPU times: user 2min 58s, sys: 4min, total: 6min 58s\n",
      "Wall time: 2min 30s\n",
      "Average test error rate: 0.3374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$batchshipyard data stream --configdir config --filespec cntkjob,run_cifar,stdout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 14:08:42,330Z DEBUG convoy.batch:stream_file_and_wait_for_task:1192 attempting to stream file stderr.txt from job=cntkjob task=run_cifar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$batchshipyard data stream --configdir config --filespec cntkjob,run_cifar,stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete job information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 19:45:46,279Z INFO convoy.batch:del_jobs:652 Deleting job: cntkjob\n",
      "2017-02-23 19:45:46,352Z DEBUG convoy.batch:del_jobs:660 waiting for job cntkjob to delete\n"
     ]
    }
   ],
   "source": [
    "!$batchshipyard jobs del -y --configdir config --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete pool (deallocate VMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-23 19:46:19,762Z INFO convoy.batch:del_pool:603 Deleting pool: scikit\n",
      "2017-02-23 19:46:19,903Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyarddht\n",
      "2017-02-23 19:46:20,002Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyardregistry\n",
      "2017-02-23 19:46:20,052Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyardtorrentinfo\n",
      "2017-02-23 19:46:20,060Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyardimages\n",
      "2017-02-23 19:46:20,081Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyardgr\n",
      "2017-02-23 19:46:20,101Z DEBUG convoy.storage:_clear_table:413 clearing table (pk=msbatchex$scikit): shipyardperf\n",
      "2017-02-23 19:46:20,110Z DEBUG convoy.storage:delete_storage_containers:368 deleting container: shipyardrf-msbatchex-scikit\n",
      "2017-02-23 19:46:20,298Z DEBUG convoy.storage:delete_storage_containers:368 deleting container: shipyardtor-msbatchex-scikit\n",
      "2017-02-23 19:46:20,311Z DEBUG convoy.storage:delete_storage_containers:374 deleting queue: shipyardgr-msbatchex-scikit\n"
     ]
    }
   ],
   "source": [
    "!$batchshipyard pool del -y --configdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarting long running operation 'Starting group delete'\u001b[0m\n",
      "\u001b[32mLong running operation 'Starting group delete' completed with result None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az group delete -n $group_name --yes --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarting long running operation 'Starting group delete'\u001b[0m\n",
      "\u001b[32mLong running operation 'Starting group delete' completed with result None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az group delete -n $docker_registry_group --yes --verbose"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:shipyard]",
   "language": "python",
   "name": "conda-env-shipyard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
